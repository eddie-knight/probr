This is supposed to be the probr core process in charge of:
- Loading config vars
- Downloading service packs
- Executing all service packs

TODO list for plugin arhitecture

|Component|Task|Description|Effort|Impact|
|---|---|---|---|---|
|Core| Create host (core) process | Implement a host process that can be used as the entry point and perform following basic functions: 1. dynamically load all available service packs; 2. execute all service packs | M | Critical |
|Core| Load config values for core process | Provide ability to set global config values for the core process | M | Critical |
|Core| Load available plugins into core process | As initial discovery mechanism, we could set available plugins in config settings and have core process load them, assuming binaries are available in some location. More elaborate discovery mechanism can be implemented later, such as automatic discovery (available in Terraform implementation) | S | Critical |
|Core| Parse available plugins and launch execution | From the core process, we need to loop thru all available plugins and launch execution using generic Service Pack interface | M | Critical |
|Core| Centralize test results from all probes | Current audit logs are saved to output path defined in config settings. Do we want any additional centralized output logs from all service packs into core? | L | Important |
|Core| Return exit code based on test results | Core process will always run all service packs regardless of errors. This is a good thing since it protects stability agains rogue implementations. Do we want to deliberately return exit codes? For instance: 0 if all service packs succeeded, 1 if any of the service packs failed, 2 due to internal errors, etc. This is essential if probr core is executed as part of a CI pipeline. | M | Important |
|Core| Design & implement a good discovery strategy | How to discover available service packs? 1. by reading explicit list in config file? 2. By discovering binaries in a known location? (I believe Terraform does this) 3. Another approach?  | M | Nice to Have |
|Core| Design & implement a good delivery strategy | How to download/install available service packs? 1. By manually dropping files into known location? 2. By downloading from repo into known location? ([see TF discovery process](https://www.terraform.io/docs/extend/how-terraform-works.html)) 3. Another approach?  | L | Nice to Have |
|Core| Implement user-friendly cli commands | In addition to executing all probes, allow users to interact with core process cli to get additional information. E.g: 1-`probr init` (discovers and donwloads available service packs) 2-`probr list servicepacks` (show all service packs available) 3- `probr help` (information about how to use the tool and available args) 4- `probr describe servicepack _spname_` (provide description about service pack what/how/requiredargs), etc | M | Nice to Have |
|Probr| Convert executable into plugin | Need to add plugin boilerplate to main function in order to be able to serve plugin over RPC connection.  | S | Critical |
|Probr| Refactor logging approach using central and injectable logger | Currently we are calling log.Printf and log.Fatalf directly to output. This approach causes conflict with RPC client-server communication and breaks the plugin execution. It also restricts the ability to change logger implementation. Proposed change is to create a global logger instance that can be swapped by hclog.Logger, or anything else. Avoid invoking log.* directly and use logger instance methods logger.Debug, logger.Trace, logger.Error, etc. Inject hclog.Logger for consistent output between host and plugin. | M | Critical |
|Probr| Inject config vars from core into each service pack | Currently, service packs should be able to continue to load settings as long as the config file and/or env vars are available. However this approach requires service packs to be "aware" of the entire config vars, as opposed to only dealing with specific configs (e.g: kubernetes sp should not care about storage settings). Another caveat to the current approach is that it requires a good deal of care and synchronization when developing new service packs to ensure proper integration. The proposed change is to refactor service pack logic to only deal with its relevant config settings, then we could have core process load settings and pass them in runtime. (e.g similar to Terraform's provider vars) | L | Important |
|Probr| Create a separate binary for each service pack | With some cleanup effort involved ahead of time, we can simply duplicate the current probr repo into multiple ones, then remove unneeded logic. For instance: 1. Create two copies of probr repo: _/citihub/probr-servicepack-kubernetes_ and _/citihub/probr-servicepack-apim_; 2. Only keep relevant code to corresponding domain (k8s or apim) and remove the rest. We can also consider having multiple service packs bundled into one plugin, which would behave as our "built-in" features. | S | Important |
|Probr| Cleanup and remove obsolete logic | Since service packs will become plugins they won't be standalone executables anymore (unless we want them to be with some feature flags). Therefore all the logic relevant to cli interaction could be removed or moved to core process (with the exception of cli flags handlers for loading config settings) | L | Nice to Have |
|Probr SDK| Create a probr SDK | Create a separate repo _/citihub/probr-sdk_ Create a common package that can be consumed by both core process and all service pack plugins. Add common logic such as plugin interface, logging, auditing, etc | M | Critical |
|Probr SDK| Enhance probr SDK to abstract underlying plugin implementation | As a service pack developer, it would be nice if I can focus on specific domain logic without worrying about learning how go-plugin package works behind the scenes. I would like something like: reference SDK, then serve this plugin. Check [Google's Terraform provider](https://github.com/hashicorp/terraform-provider-google) | L | Nice to Have |